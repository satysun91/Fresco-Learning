{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Using Scikit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization\n",
    "Standardization or Mean Removal is the process of transforming each feature vector into a normal distribution with mean 0 and variance 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.datasets as datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of each feature after Standardization :\n",
      "\n",
      "\n",
      "[-3.16286735e-15 -6.53060890e-15 -7.07889127e-16 -8.79983452e-16\n",
      "  6.13217737e-15 -1.12036918e-15 -4.42138027e-16  9.73249991e-16\n",
      " -1.97167024e-15 -1.45363120e-15 -9.07641468e-16 -8.85349205e-16\n",
      "  1.77367396e-15 -8.29155139e-16 -7.54180940e-16 -3.92187747e-16\n",
      "  7.91789988e-16 -2.73946068e-16 -3.10823423e-16 -3.36676596e-16\n",
      " -2.33322442e-15  1.76367415e-15 -1.19802625e-15  5.04966114e-16\n",
      " -5.21317026e-15 -2.17478837e-15  6.85645643e-16 -1.41265636e-16\n",
      " -2.28956670e-15  2.57517109e-15]\n",
      "\n",
      "Std. of each feature after Standardization :\n",
      "\n",
      "\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "breast_cancer=datasets.load_breast_cancer()\n",
    "standardizer = preprocessing.StandardScaler()\n",
    "standardizer = standardizer.fit(breast_cancer.data)\n",
    "breast_cancer_standardized = standardizer.transform(breast_cancer.data)\n",
    "print('Mean of each feature after Standardization :\\n\\n')\n",
    "print(breast_cancer_standardized.mean(axis=0))\n",
    "print('\\nStd. of each feature after Standardization :\\n\\n')\n",
    "print(breast_cancer_standardized.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling\n",
    "Scaling transforms existing data values to lie between a minimum and maximum value.\n",
    "\n",
    "MinMaxScaler transforms data to range 0 and 1.\n",
    "\n",
    "MaxAbsScaler transforms data to range -1 and 1.\n",
    "\n",
    "Transforming breast_cancer dataset through Scaling is shown in next three cards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MinMaxScaler\n",
    "MinMaxScaler with specified range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 10)).fit(breast_cancer.data)\n",
    "\n",
    "breast_cancer_minmaxscaled10 = min_max_scaler.transform(breast_cancer.data)\n",
    "#In the above example, data is transformed to range 0 and 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MaxAbsScaler\n",
    "Using MaxAbsScaler, the maximum absolute value of each feature is scaled to unit size, i.e., 1. It is intended for data that is previously centered at sparse or zero data.\n",
    "\n",
    "Example for MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_abs_scaler = preprocessing.MaxAbsScaler().fit(breast_cancer.data)\n",
    "\n",
    "breast_cancer_maxabsscaled = max_abs_scaler.transform(breast_cancer.data)\n",
    "#By default, MaxAbsScaler transforms data to the range -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "Normalization scales each sample to have a unit norm.\n",
    "\n",
    "Normalization can be achieved with 'l1', 'l2', and 'max' norms.\n",
    "\n",
    "'l1' norm makes the sum of absolute values of each row as 1, and 'l2' norm makes the sum of squares of each row as 1.\n",
    "\n",
    "'l1' norm is insensitive to outliers.\n",
    "\n",
    "By default l2 norm is considered. Hence, removing outliers is recommended before applying l2 norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization - Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = preprocessing.Normalizer(norm='l1').fit(breast_cancer.data)\n",
    "\n",
    "breast_cancer_normalized = normalizer.transform(breast_cancer.data)\n",
    "#In above example, l1 norm is used with norm parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binarization\n",
    "Binarization is the process of transforming data points to 0 or 1 based on a given threshold.\n",
    "\n",
    "Any value above the threshold is transformed to 1, and any value below the threshold is transformed to 0.\n",
    "By default, a threshold of 0 is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binarization - Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "binarizer = preprocessing.Binarizer(threshold=3.0).fit(breast_cancer.data)\n",
    "breast_cancer_binarized = binarizer.transform(breast_cancer.data)\n",
    "print(breast_cancer_binarized[:5,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneHotEncoder\n",
    "OneHotEncoder converts categorical integer values into one-hot vectors. In an on-hot vector, every category is transformed into a binary attribute having only 0 and 1 values.\n",
    "\n",
    "An example creating two binary attributes for the categorical integers 1 and 2, is shown in the next slide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneHotEncoder - Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]]\n",
      "[[0. 1.]]\n",
      "[[1. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP Pavilion\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "onehotencoder = preprocessing.OneHotEncoder()\n",
    "onehotencoder = onehotencoder.fit([[1], [1], [1], [2], [2], [1]])\n",
    "\n",
    "# Transforming category values 1 and 2 to one-hot vectors\n",
    "print(onehotencoder.transform([[1]]).toarray())\n",
    "print(onehotencoder.transform([[2]]).toarray())\n",
    "print(onehotencoder.transform([[1]]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation\n",
    "Imputation replaces missing values with either median, mean, or the most common value of the column or row in which the missing values exist.\n",
    "\n",
    "Below example replaces missing values, represented by np.nan, with the mean of respective column (axis 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP Pavilion\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "imputer = preprocessing.Imputer(missing_values='NaN', strategy='mean')\n",
    "\n",
    "imputer = imputer.fit(breast_cancer.data)\n",
    "breast_cancer_imputed = imputer.transform(breast_cancer.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoding\n",
    "Label Encoding is a step in which, in which categorical features are represented as categorical integers. An example of transforming categorical values [\"benign\",\"malignant\"]into[0, 1]` is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['malignant', 'benign', 'malignant', 'benign']\n",
    "\n",
    "labelencoder = preprocessing.LabelEncoder()\n",
    "\n",
    "labelencoder = labelencoder.fit(labels)\n",
    "\n",
    "bc_labelencoded = labelencoder.transform(breast_cancer.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler().fit(breast_cancer.data)\n",
    "\n",
    "breast_cancer_minmaxscaled = min_max_scaler.transform(breast_cancer.data)\n",
    "#By default, transformation occurs to a range of 0 and 1. It can also be customized with feature_range argument as shown in next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Using Scikit-Learn | 5 | SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "Import two modules sklearn.datasets, and sklearn.model_selection.\n",
    "\n",
    "Load popular digits dataset from sklearn.datasets module and assign it to variable digits.\n",
    "\n",
    "Split digits.data into two sets names X_train and X_test. Also, split digits.target into two sets Y_train and Y_test.\n",
    "\n",
    "Hint: Use train_test_split method from sklearn.model_selection; set random_state to 30; and perform stratified sampling.\n",
    "Print the shape of X_train dataset.\n",
    "\n",
    "Print the shape of X_test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1347, 64)\n",
      "(450, 64)\n"
     ]
    }
   ],
   "source": [
    "digits=datasets.load_digits()\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(digits.data,digits.target,random_state=30, stratify=digits.target)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Import required module from sklearn.svm.\n",
    "\n",
    "Build an SVM classifier from X_train set and Y_train labels, with default parameters. Name the model as svm_clf.\n",
    "\n",
    "Evaluate the model accuracy on testing data set and print it's score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP Pavilion\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Data : 0.6022222222222222\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.svm import SVC\n",
    "digits=datasets.load_digits()\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(digits.data,digits.target,random_state=30, stratify=digits.target)\n",
    "svm_classifier = SVC()\n",
    "svm_clf= svm_classifier.fit(X_train, Y_train) \n",
    "print('Accuracy of Test Data :', svm_classifier.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "Perform Standardization of digits.data and store the transformed data in variable digits_standardized.\n",
    "\n",
    "Hint : Use required utility from sklearn.preprocessing.\n",
    "Once again, split digits_standardized into two sets names X_train and X_test. Also, split digits.target into two sets Y_train and Y_test.\n",
    "\n",
    "Hint: Use train_test_split method from sklearn.model_selection; set random_state to 30; and perform stratified sampling.\n",
    "Build another SVM classifier from X_train set and Y_train labels, with default parameters. Name the model as svm_clf2.\n",
    "\n",
    "Evaluate the model accuracy on testing data set and print it's score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP Pavilion\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Data : 0.6022222222222222\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.datasets as datasets\n",
    "digits=datasets.load_digits()\n",
    "standardizer = preprocessing.StandardScaler()\n",
    "standardizer = standardizer.fit(digits.data)\n",
    "cancer_standardized = standardizer.transform(digits.data)\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(digits.data,digits.target,random_state=30, stratify=digits.target)\n",
    "svm_classifier = SVC()\n",
    "svm_classifier = svm_classifier.fit(X_train, Y_train) \n",
    "#print('Accuracy of Train Data :', svm_classifier.score(X_train,Y_train))\n",
    "print('Accuracy of Test Data :', svm_classifier.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP Pavilion\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Train Data : 1.0\n",
      "Accuracy of Test Data : 0.4622222222222222\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-cbcd3d610c64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mcancer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_breast_cancer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mstandardizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mstandardizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcancer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mcancer_standardized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcancer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_classifier = SVC()\n",
    "\n",
    "svm_classifier = svm_classifier.fit(X_train, Y_train) \n",
    "\n",
    "print('Accuracy of Train Data :', svm_classifier.score(X_train,Y_train))\n",
    "\n",
    "print('Accuracy of Test Data :', svm_classifier.score(X_test,Y_test))\n",
    "\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.datasets as datasets\n",
    "cancer=datasets.load_breast_cancer\n",
    "standardizer = preprocessing.StandardScaler()\n",
    "standardizer = standardizer.fit(cancer.data)\n",
    "cancer_standardized = standardizer.transform(cancer.data)\n",
    "\n",
    "svm_classifier = SVC()\n",
    "\n",
    "svm_classifier = svm_classifier.fit(X_train, Y_train) \n",
    "print('Accuracy of Train Data :', svm_classifier.score(X_train,Y_train))\n",
    "\n",
    "print('Accuracy of Test Data :', svm_classifier.score(X_test,Y_test))\n",
    "from sklearn import metrics\n",
    "\n",
    "Y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "print('Classification report : \\n',metrics.classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "cancer = datasets.load_breast_cancer()  # Loading the data set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cancer.data, cancer.target,\n",
    "           stratify=cancer.target,                  random_state=42)\n",
    "    \n",
    "knn_classifier = KNeighborsClassifier()   \n",
    "\n",
    "knn_classifier = knn_classifier.fit(X_train, Y_train) \n",
    "print('Accuracy of Train Data :', knn_classifier.score(X_train,Y_train))\n",
    "print('Accuracy of Test Data :', knn_classifier.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "Import two modules sklearn.datasets, and sklearn.model_selection.\n",
    "\n",
    "Load popular iris data set from sklearn.datasets module and assign it to variable iris.\n",
    "\n",
    "Split iris.data into two sets names X_train and X_test. Also, split iris.target into two sets Y_train and Y_test.\n",
    "\n",
    "Hint: Use train_test_split method from sklearn.model_selection; set random_state to 30 and perform stratified sampling.\n",
    "Print the shape of X_train dataset.\n",
    "\n",
    "Print the shape of X_test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 4)\n",
      "(38, 4)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np\n",
    "iris=datasets.load_iris()\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(iris.data,iris.target,random_state=30, stratify=iris.target)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Import required module from sklearn.neighbors\n",
    "\n",
    "Fit K nearest neighbors model on X_train data and Y_train labels, with default parameters. Name the model as knn_clf.\n",
    "\n",
    "Evaluate the model accuracy on training data set and print it's score.\n",
    "\n",
    "Evaluate the model accuracy on testing data set and print it's score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Train Data : 0.9821428571428571\n",
      "Accuracy of Test Data : 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "iris=datasets.load_iris()\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(iris.data,iris.target,random_state=30, stratify=iris.target)\n",
    "\n",
    "knn_classifier = KNeighborsClassifier()   \n",
    "\n",
    "knn_classifier = knn_classifier.fit(X_train, Y_train) \n",
    "print('Accuracy of Train Data :', knn_classifier.score(X_train,Y_train))\n",
    "print('Accuracy of Test Data :', knn_classifier.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "Fit multiple K nearest neighbors models on X_train data and Y_train labels with n_neighbors parameter value changing from 3 to 10.\n",
    "\n",
    "Evaluate each model accuracy on testing data set.\n",
    "\n",
    "Hint: Make use of for loop\n",
    "Print the n_neighbors value of the model with highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-636e6443a269>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mknn_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mknn_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mknn_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mknn_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;31m#print('Accuracy of Test Data :',a)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "iris=datasets.load_iris()\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(iris.data,iris.target,random_state=30, stratify=iris.target)\n",
    "a=[]\n",
    "for i in range(3,11):\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=i)   \n",
    "    knn_classifier = knn_classifier.fit(X_train, Y_train) \n",
    "    a[i].append(knn_classifier.score(X_test,Y_test))\n",
    "    #print('Accuracy of Test Data :',a)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "Import two modules sklearn.datasets and sklearn.preprocessing.\n",
    "\n",
    "Load popular iris data set from sklearn.datasets module and assign it to variable 'iris'.\n",
    "\n",
    "Perform Normalization on iris.data with l2 norm and save the transformed data in variable iris_normalized.\n",
    "\n",
    "Hint: Use Normalizer API.\n",
    "Print the mean of every column using the below command. print(iris_normalized.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75140029 0.40517418 0.45478362 0.14107142]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as datasets\n",
    "import sklearn.preprocessing as preprocessing\n",
    "iris=datasets.load_iris()\n",
    "normalizer = preprocessing.Normalizer(norm='l2').fit(iris.data)\n",
    "iris_normalized = normalizer.transform(iris.data)\n",
    "print(iris_normalized.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Convert the categorical integer list iris.target into three binary attribute representation and store the result in variable iris_target_onehot.\n",
    "\n",
    "Hint: Use reshape(-1,1) on iris.target and OneHotEncoder.\n",
    "Execute the following print statement print(iris_target_onehot.toarray()[[0,50,100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP Pavilion\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as datasets\n",
    "import sklearn.preprocessing as preprocessing\n",
    "iris=datasets.load_iris()\n",
    "iris_target_onehot=iris.target.reshape(-1,1)\n",
    "onehotencoder = preprocessing.OneHotEncoder()\n",
    "iris_target_onehot= onehotencoder.fit_transform(iris_target_onehot).toarray()\n",
    "#print(onehotencoder.transform([[2]]).toarray())\n",
    "#print(iris_target_onehot[[0,50,100]])\n",
    "#print(onehotencoder.transform([[1]]).toarray())\n",
    "print(iris_target_onehot[[0,50,100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "Set first 50 row values of iris.data to Null values. Use numpy.nan\n",
    "\n",
    "Perform Imputation on 'iris.data' and save the transformed data in variable 'iris_imputed'.\n",
    "\n",
    "Hint : use Imputer API, Replace numpy.NaN values with mean of corresponding data.\n",
    "Print the mean of every column using the below command. print(iris_imputed.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.262 2.872 4.906 1.676]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP Pavilion\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as datasets\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import numpy as np\n",
    "iris=datasets.load_iris()\n",
    "iris.data[0:50]=np.nan\n",
    "imputer = preprocessing.Imputer(missing_values='NaN', strategy='mean')\n",
    "imputer = imputer.fit(iris.data)\n",
    "iris_imputed= imputer.transform(iris.data)\n",
    "print(iris_imputed.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Decision Tree Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier()   \n",
    "\n",
    "dt_classifier = dt_classifier.fit(X_train, Y_train) \n",
    "print('Accuracy of Train Data :', dt_classifier.score(X_train,Y_train))\n",
    "\n",
    "print('Accuracy of Test Data :', dt_classifier.score(X_test,Y_test))\n",
    "dt_classifier = DecisionTreeClassifier(max_depth=2)   \n",
    "\n",
    "dt_classifier = dt_classifier.fit(X_train, Y_train) \n",
    "\n",
    "print('Accuracy of Train Data :', dt_classifier.score(X_train,Y_train))\n",
    "\n",
    "print('Accuracy of Test Data :', dt_classifier.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "Import two modules sklearn.datasets, and sklearn.model_selection.\n",
    "Import numpy and set random seed to 100.\n",
    "\n",
    "Load popular Boston dataset from sklearn.datasets module and assign it to variable boston.\n",
    "\n",
    "Split boston.data into two sets names X_train and X_test. Also, split boston.target into two sets Y_train and Y_test.\n",
    "\n",
    "Hint: Use train_test_split method from sklearn.model_selection; set random_state to 30.\n",
    "Print the shape of X_train dataset.\n",
    "\n",
    "Print the shape of X_test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 13)\n",
      "(127, 13)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np\n",
    "np.random.seed(100)\n",
    "boston=datasets.load_boston()\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(boston.data,boston.target,random_state=30)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Import required module from sklearn.tree.\n",
    "\n",
    "Build a Decision tree Regressor model from X_train set and Y_train labels, with default parameters. Name the model as dt_reg.\n",
    "\n",
    "Evaluate the model accuracy on training data set and print it's score.\n",
    "\n",
    "Evaluate the model accuracy on testing data set and print it's score.\n",
    "\n",
    "Predict the housing price for first two samples of X_test set and print them.(Hint : Use predict() function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Train Data : 1.0\n",
      "Accuracy of Test Data : 0.7038312015987166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([18.2, 13.9])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np\n",
    "np.random.seed(100)\n",
    "boston=datasets.load_boston()\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(boston.data,boston.target,random_state=30)\n",
    "\n",
    "dt_reg = DecisionTreeRegressor()   \n",
    "dt_reg = dt_reg.fit(X_train, Y_train) \n",
    "print('Accuracy of Train Data :', dt_reg.score(X_train,Y_train))\n",
    "print('Accuracy of Test Data :', dt_reg.score(X_test,Y_test))\n",
    "dt_reg.predict(X_test[0:2,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(max_depth=2)   \n",
    "\n",
    "dt_classifier = dt_reg.fit(X_train, Y_train) \n",
    "\n",
    "print('Accuracy of Train Data :', dt_reg.score(X_train,Y_train))\n",
    "\n",
    "print('Accuracy of Test Data :', dt_reg.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7805024671754182\n",
      "0.10550617628787605\n",
      "0.18588493803637118\n",
      "0.00027782071833537086\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans_cluster = KMeans(n_clusters=2)\n",
    "\n",
    "kmeans_cluster = kmeans_cluster.fit(X_train) \n",
    "\n",
    "kmeans_cluster.predict(X_test)\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.homogeneity_score(kmeans_cluster.predict(X_test), Y_test))\n",
    "\n",
    "print(metrics.completeness_score(kmeans_cluster.predict(X_test), Y_test))\n",
    "\n",
    "print(metrics.v_measure_score(kmeans_cluster.predict(X_test), Y_test))\n",
    "\n",
    "print(metrics.adjusted_rand_score(kmeans_cluster.predict(X_test), Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "Import three modules sklearn.datasets, sklearn.cluster, and sklearn.metrics.\n",
    "\n",
    "Load popular iris dataset from sklearn.datasets module and assign it to variable iris.\n",
    "\n",
    "Cluster iris.data set into 3 clusters using K-means with default parameters. Name the model as km_cls.\n",
    "\n",
    "Hint : Import required utility from sklearn.cluster\n",
    "Determine the homogeneity score of the model and print it.\n",
    "\n",
    "Hint : Import required utility from sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7312509997670167\n",
      "0.7187287275730635\n",
      "0.7249357914309795\n",
      "0.6831077974414224\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import sklearn.datasets as datasets\n",
    "iris=datasets.load_iris()\n",
    "\n",
    "km_cls= KMeans(n_clusters=3)\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(iris.data,iris.target)\n",
    "km_cls = km_cls.fit(X_train) \n",
    "\n",
    "kmeans_cluster.predict(X_test)\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.homogeneity_score(km_cls.predict(X_test), Y_test))\n",
    "\n",
    "print(metrics.completeness_score(km_cls.predict(X_test), Y_test))\n",
    "\n",
    "print(metrics.v_measure_score(km_cls.predict(X_test), Y_test))\n",
    "\n",
    "print(metrics.adjusted_rand_score(km_cls.predict(X_test), Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Cluster iris.data set into 3 clusters using Agglomerative clustering. Name the model as agg_cls.\n",
    "\n",
    "Hint : Import required utility from sklearn.cluster\n",
    "Determine the homogeneity score of the model and print it.\n",
    "\n",
    "Hint : Import required utility from sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7608008469718723"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "iris=datasets.load_iris()\n",
    "\n",
    "\n",
    "agg_cls = AgglomerativeClustering(n_clusters=3)\n",
    "agg_cls.fit(iris.data)\n",
    "labels2=agg_cls.labels_\n",
    "homogeneity_score(iris.target,labels2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "Cluster iris.data set using Affinity Propagation clustering method with default parameters. Name the model as af_cls.\n",
    "\n",
    "Hint : Import required utility from sklearn.cluster\n",
    "Determine the homogeneity score of the model and print it.\n",
    "\n",
    "Hint : Import required utility from sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7608008469718723"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "iris=datasets.load_iris()\n",
    "af_cls= AffinityPropagation(preference=-50)\n",
    "af_cls.fit(iris.data)\n",
    "labels2=agg_cls.labels_\n",
    "homogeneity_score(iris.target,labels2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Train Data : 0.9910714285714286\n",
      "Accuracy of Test Data : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP Pavilion\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "rf_classifier = rf_classifier.fit(X_train, Y_train) \n",
    "\n",
    "print('Accuracy of Train Data :', rf_classifier.score(X_train,Y_train))\n",
    "\n",
    "print('Accuracy of Test Data :', rf_classifier.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Import required module from sklearn.ensemble.\n",
    "\n",
    "Build a Random Forest Regressor model from X_train set and Y_train labels, with default parameters. Name the model as rf_reg.\n",
    "\n",
    "Evaluate the model accuracy on training data set and print it's score.\n",
    "\n",
    "Evaluate the model accuracy on testing data set and print it's score.\n",
    "\n",
    "Predict the housing price for first two samples of X_test set and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Train Data : 0.9735273853589964\n",
      "Accuracy of Test Data : 0.8829204928067821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP Pavilion\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([19.59,  9.47])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np\n",
    "np.random.seed(100)\n",
    "boston=datasets.load_boston()\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(boston.data,boston.target,random_state=30)\n",
    "\n",
    "rf_reg= RandomForestRegressor()\n",
    "rf_reg = rf_reg.fit(X_train, Y_train) \n",
    "print('Accuracy of Train Data :', rf_reg.score(X_train,Y_train))\n",
    "print('Accuracy of Test Data :', rf_reg.score(X_test,Y_test))\n",
    "rf_reg.predict(X_test[0:2,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "Build multiple Random forest regressor on X_train set and Y_train labels with max_depth parameter value changing from 3 to 5 and also setting n_estimators to one of 50, 100, 200 values.\n",
    "\n",
    "Evaluate each model accuracy on testing data set.\n",
    "\n",
    "Hint: Make use of for loop\n",
    "Print the max_depth and n_estimators values of the model with highest accuracy.\n",
    "\n",
    "Note: Print the parameter values in the form of tuple (a, b). a refers to max_depth value and b refers to n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Train Data : 0.9710420861029339\n",
      "Accuracy of Test Data : 0.8799361210962687\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np\n",
    "np.random.seed(100)\n",
    "boston=datasets.load_boston()\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(boston.data,boston.target,random_state=30)\n",
    "rf_reg=RandomForestRegressor(random_state = 1, n_estimators = 10)\n",
    "rf_reg =rf_reg.fit(X_train, Y_train) \n",
    "print('Accuracy of Train Data :', rf_reg.score(X_train,Y_train))\n",
    "print('Accuracy of Test Data :', rf_reg.score(X_test,Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
